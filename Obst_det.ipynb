{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Obstacle detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install -q --use-deprecated=legacy-resolver tflite-model-maker\n",
        "!pip install -q pycocotools\n",
        "!pip install -q opencv-python-headless==4.1.2.30\n",
        "!pip uninstall -y tensorflow && pip install -q tensorflow==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import QuantizationConfig\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn61LJ9QbOPi"
      },
      "source": [
        "\n",
        "\n",
        "**object detection model archiecture.**\n",
        "\n",
        "\n",
        "Performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtdZ-JDwMimd"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite4')  #selected lite4 version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5U-A3tw6Y27"
      },
      "source": [
        "**Dataset loading**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD5BvzWe6YKa"
      },
      "outputs": [],
      "source": [
        "train_data, validation_data, test_data = object_detector.DataLoader.from_csv('gs://navigation_detection.csv')  #load the dataset csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "**Training**\n",
        "\n",
        "* The EfficientDet-Lite0 model uses `epochs = 50` by default\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwlYdTcg63xy"
      },
      "outputs": [],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzCHLWJ6h7q"
      },
      "source": [
        "**Evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xmnl6Yy7ARn"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "**Export as a TensorFlow Lite model**\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format.The default post-training quantization technique is full integer quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm_UULdW7A9T"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQpahAIBqBPp"
      },
      "source": [
        "**Evaluation of the TensorFlow Lite model.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS3Ell_lqH4e"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('model.tflite', test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XqS0rFCrqM1o"
      },
      "outputs": [],
      "source": [
        "#@title Load the trained TFLite model and define some visualization functions\n",
        "\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "model_path = 'model.tflite'\n",
        "\n",
        "# Load the labels into a list\n",
        "classes = ['???'] * model.model_spec.config.num_classes\n",
        "label_map = model.model_spec.config.label_map\n",
        "for label_id, label_name in label_map.as_dict().items():\n",
        "  classes[label_id-1] = label_name\n",
        "\n",
        "# Define a list of colors for visualization\n",
        "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
        "\n",
        "def preprocess_image(image_path, input_size):\n",
        "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "  original_image = img\n",
        "  resized_img = tf.image.resize(img, input_size)\n",
        "  resized_img = resized_img[tf.newaxis, :]\n",
        "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
        "  return resized_img, original_image\n",
        "\n",
        "\n",
        "def detect_objects(interpreter, image, threshold):\n",
        "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
        "\n",
        "  signature_fn = interpreter.get_signature_runner()\n",
        "\n",
        "  # Feed the input image to the model\n",
        "  output = signature_fn(images=image)\n",
        "\n",
        "  # Get all outputs from the model\n",
        "  count = int(np.squeeze(output['output_0']))\n",
        "  scores = np.squeeze(output['output_1'])\n",
        "  classes = np.squeeze(output['output_2'])\n",
        "  boxes = np.squeeze(output['output_3'])\n",
        "\n",
        "  results = []\n",
        "  for i in range(count):\n",
        "    if scores[i] >= threshold:\n",
        "      result = {\n",
        "        'bounding_box': boxes[i],\n",
        "        'class_id': classes[i],\n",
        "        'score': scores[i]\n",
        "      }\n",
        "      results.append(result)\n",
        "  return results\n",
        "\n",
        "\n",
        "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
        "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
        "  # Load the input shape required by the model\n",
        "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
        "\n",
        "  # Load the input image and preprocess it\n",
        "  preprocessed_image, original_image = preprocess_image(\n",
        "      image_path,\n",
        "      (input_height, input_width)\n",
        "    )\n",
        "\n",
        "  # Run object detection on the input image\n",
        "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
        "\n",
        "  # Plot the detection results on the input image\n",
        "  original_image_np = original_image.numpy().astype(np.uint8)\n",
        "  for obj in results:\n",
        "    # Convert the object bounding box from relative coordinates to absolute\n",
        "    # coordinates based on the original image resolution\n",
        "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "    xmin = int(xmin * original_image_np.shape[1])\n",
        "    xmax = int(xmax * original_image_np.shape[1])\n",
        "    ymin = int(ymin * original_image_np.shape[0])\n",
        "    ymax = int(ymax * original_image_np.shape[0])\n",
        "\n",
        "    # Find the class index of the current object\n",
        "    class_id = int(obj['class_id'])\n",
        "\n",
        "    # Draw the bounding box and label on the image\n",
        "    color = [int(c) for c in COLORS[class_id]]\n",
        "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "    # Make adjustments to make the label visible for all objects\n",
        "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
        "    cv2.putText(original_image_np, label, (xmin, y),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "  # Return the final image\n",
        "  original_uint8 = original_image_np.astype(np.uint8)\n",
        "  return original_uint8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWiA_zX8rxE"
      },
      "source": [
        "## Additionals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8VxPiOLy4Gv"
      },
      "source": [
        "### Customize the EfficientDet model hyperparameters\n",
        "\n",
        "The model and training pipline parameters you can adjust are:\n",
        "\n",
        "* `model_dir`: The location to save the model checkpoint files. If not set, a temporary directory will be used.\n",
        "* `steps_per_execution`: Number of steps per training execution.\n",
        "* `moving_average_decay`: Float. The decay to use for maintaining moving averages of the trained parameters.\n",
        "* `var_freeze_expr`: The regular expression to map the prefix name of variables to be frozen which means remaining the same during training. More specific, use `re.match(var_freeze_expr, variable_name)` in the codebase to map the variables to be frozen.\n",
        "* `tflite_max_detections`: integer, 25 by default. The max number of output detections in the TFLite model.\n",
        "* `debug`: Enable debug mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQuy7RSDir3"
      },
      "source": [
        "### Tune the training hyperparameters\n",
        "\n",
        "The `create` function is the driver function that the Model Maker library uses to create models. The `create` function comprises of the following steps:\n",
        "\n",
        "1. Creates the model for the object detection according to `model_spec`.\n",
        "2. Trains the model.  The default epochs and the default batch size are set by the `epochs` and `batch_size` variables in the `model_spec` object.\n",
        "You can also tune the training hyperparameters like `epochs` and `batch_size` that affect the model accuracy. For instance,\n",
        "\n",
        "*   `epochs`: Integer, 50 by default. More epochs could achieve better accuracy, but may lead to overfitting.\n",
        "*   `batch_size`: Integer, 64 by default. The number of samples to use in one training step.\n",
        "*   `train_whole_model`: Boolean, False by default. If true, train the whole model. Otherwise, only train the layers that do not match `var_freeze_expr`.\n",
        "\n",
        "```python\n",
        "model = object_detector.create(train_data, model_spec=spec, epochs=10, validation_data=validation_data)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vPyZInPxJBT"
      },
      "source": [
        "### Export to different formats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhsZhW3ApcX"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "model.export(export_dir='.')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5q_McchQ2C4"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "```python\n",
        "config = QuantizationConfig.for_float16()\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "IMAGE_SHAPE = (300, 300)\n",
        "TRAINING_DATA_DIR = ''\n",
        "\n",
        "datagen_kwargs = dict(rescale=1./255)\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\n",
        "test_generator = test_datagen.flow_from_directory(TRAINING_DATA_DIR, shuffle=True,target_size=IMAGE_SHAPE,batch_size=1)\n",
        "\n",
        "\n",
        "# Learn about dataset labels\n",
        "dataset_labels = sorted(test_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\n",
        "print(dataset_labels)\n",
        "\n",
        "\n",
        "true_= None\n",
        "for i in range(7577):\n",
        "  # Get images and labels batch from test dataset generator\n",
        "  test_image_batch, test_label_batch = next(iter(test_generator))\n",
        "  if i==0:\n",
        "    true_ = test_label_batch\n",
        "  else:\n",
        "    true_ = np.append(true_, test_label_batch, axis=0)\n",
        "  true_label_ids = np.argmax(test_label_batch, axis=-1)\n",
        "  #print(\"test batch shape:\", test_image_batch.shape)\n",
        "  print('Testing on ',(i+1)*1)\n",
        "\n",
        "  tflite_interpreter = tf.lite.Interpreter(model_path='model.tflite')\n",
        "\n",
        "  input_details = tflite_interpreter.get_input_details()\n",
        "  output_details = tflite_interpreter.get_output_details()\n",
        "\n",
        "  tflite_interpreter.resize_tensor_input(input_details[0]['index'], (1, 300, 300, 3))\n",
        "  tflite_interpreter.resize_tensor_input(output_details[0]['index'], (1, 15))\n",
        "  tflite_interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = tflite_interpreter.get_input_details()\n",
        "  output_details = tflite_interpreter.get_output_details()\n",
        "\n",
        "  tflite_interpreter.set_tensor(input_details[0]['index'], test_image_batch)\n",
        "\n",
        "  tflite_interpreter.invoke()\n",
        "  if i==0:\n",
        "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
        "  else:\n",
        "    temp = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
        "    tflite_model_predictions = np.append(tflite_model_predictions, temp, axis=0)\n",
        "  \n",
        "  del tflite_interpreter"
      ],
      "metadata": {
        "id": "-OA0lzdEVAlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "tflite_predicted_ids1 = np.argmax(tflite_model_predictions, axis=-1)\n",
        "true_label_ids1 = np.argmax(true_, axis=-1)\n",
        "\n",
        "y_true = dataset_labels[np.argmax(true_, axis=-1)]\n",
        "y_pred = dataset_labels[np.argmax(tflite_model_predictions, axis=-1)]\n",
        "data = confusion_matrix(y_true, y_pred)\n",
        "df_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n",
        "df_cm.index.name = 'Predicted'\n",
        "df_cm.columns.name = 'Actual'\n",
        "plt.figure(figsize = (17,15))\n",
        "sn.set(font_scale=1.4)#for label size\n",
        "sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 12},fmt='g')# font size"
      ],
      "metadata": {
        "id": "6yASqFipVHDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACCURACY FOR EACH CLASS"
      ],
      "metadata": {
        "id": "7fFBDI_xVK4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accu = {}\n",
        "tflite_predicted_ids1 = np.argmax(tflite_model_predictions, axis=-1)\n",
        "true_label_ids1 = np.argmax(true_, axis=-1)\n",
        "for i in range(len(true_label_ids1)):\n",
        "  if true_label_ids1[i] == tflite_predicted_ids1[i]:\n",
        "    accu[str(dataset_labels[true_label_ids1[i]])] = 0\n",
        "\n",
        "for i in range(len(true_label_ids1)):\n",
        "  if true_label_ids1[i] == tflite_predicted_ids1[i]:\n",
        "    accu[str(dataset_labels[true_label_ids1[i]])] += 1\n",
        "\n",
        "pred = list(tflite_predicted_ids1)\n",
        "true = list(true_label_ids1)\n",
        "for i in range(15):\n",
        "  print(dataset_labels[i],'-', round(accu[dataset_labels[i]]/true.count(i)*100,2),'%')"
      ],
      "metadata": {
        "id": "rrFtufaPVNml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score"
      ],
      "metadata": {
        "id": "IhfQVJjMVSax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa\n",
        "metric = tfa.metrics.F1Score(num_classes=15, threshold=0.5)\n",
        "\n",
        "metric.update_state(true_, tflite_model_predictions)\n",
        "\n",
        "result = metric.result()\n",
        "result.numpy()"
      ],
      "metadata": {
        "id": "m_mUMrOXVTlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision"
      ],
      "metadata": {
        "id": "jbuqLZNQVYj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(15):\n",
        "  m = tf.keras.metrics.Precision(class_id=i)\n",
        "  m.update_state(true_, tflite_model_predictions)\n",
        "  print(m.result().numpy())"
      ],
      "metadata": {
        "id": "MmyyW6FDVasb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall"
      ],
      "metadata": {
        "id": "8C6pAcs2VdYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(15):\n",
        "  m = tf.keras.metrics.Recall(class_id=i)\n",
        "  m.update_state(true_, tflite_model_predictions)\n",
        "  print(m.result().numpy())"
      ],
      "metadata": {
        "id": "Tv2fp8RFVfcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}